{"cells":[{"cell_type":"markdown","id":"91443856-5b11-4357-a272-f657b9d0e981","metadata":{},"source":["## Classifying Dogs by Emotional expression using a CNN"]},{"cell_type":"markdown","metadata":{},"source":["### Imports"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# You can also use this section to suppress warnings generated by your code:\n","def warn(*args, **kwargs):\n","    pass\n","\n","\n","import warnings\n","warnings.warn = warn\n","warnings.filterwarnings('ignore')\n","\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # tensorflow INFO and WARNING messages are not printed \n","\n","import random \n","\n","import pathlib\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","import PIL\n","import PIL.Image\n","from PIL import Image, ImageOps\n","import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","import keras\n","from keras.preprocessing import image\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","\n","sns.set_context('notebook')\n","sns.set_style('white')"]},{"cell_type":"markdown","metadata":{},"source":["### Loading Images"]},{"cell_type":"code","execution_count":null,"id":"f258528c-a56b-456a-a535-4c6d065d2997","metadata":{},"outputs":[],"source":["import pathlib\n","dataset_url = \"\"\n","\n","# Download the data and track where it's saved using tf.keras.utils.get_file in a variable called data_dir\n","data_dir = keras.utils.get_file(origin=dataset_url,\n","                                   fname='dogs',\n","                                   untar=True)\n","data_dir = pathlib.Path(data_dir)\n","labels = list()\n","\n","print(data_dir)\n","for folder in data_dir.glob('*'):\n","    labels.append(folder.name)\n","    print('The', folder.name, 'folder has', len(list(folder.glob('*.jpeg'))), 'pictures')\n","image_count = len(list(data_dir.glob('*/*.jpeg')))\n","print(image_count, 'total images')"]},{"cell_type":"markdown","metadata":{},"source":["### Viewing Image Examples"]},{"cell_type":"markdown","metadata":{},"source":["#### Angry"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["content = list(data_dir.glob('angry/*'))\n","PIL.Image.open(str(content[1]))"]},{"cell_type":"markdown","metadata":{},"source":["#### Content"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["content = list(data_dir.glob('content/*'))\n","PIL.Image.open(str(content[1]))"]},{"cell_type":"markdown","metadata":{},"source":["#### Disgusted"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["disgusted = list(data_dir.glob('disgusted/*'))\n","PIL.Image.open(str(disgusted[1]))"]},{"cell_type":"markdown","metadata":{},"source":["#### Distressed"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["distressed = list(data_dir.glob('distressed/*'))\n","PIL.Image.open(str(distressed[1]))"]},{"cell_type":"markdown","metadata":{},"source":["#### Excited"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["excited = list(data_dir.glob('excited/*'))\n","PIL.Image.open(str(excited[1]))"]},{"cell_type":"markdown","metadata":{},"source":["#### Happy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["happy = list(data_dir.glob('happy/*'))\n","PIL.Image.open(str(happy[1]))"]},{"cell_type":"markdown","metadata":{},"source":["#### Loving"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loving = list(data_dir.glob('loving/*'))\n","PIL.Image.open(str(loving[1]))"]},{"cell_type":"markdown","metadata":{},"source":["#### Scared"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scared = list(data_dir.glob('scared/*'))\n","PIL.Image.open(str(scared[1]))"]},{"cell_type":"markdown","metadata":{},"source":["#### Shy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["shy = list(data_dir.glob('shy/*'))\n","PIL.Image.open(str(shy[1]))"]},{"cell_type":"markdown","metadata":{},"source":["### Processing Images"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# The batch size simply specifies the number of images to pass through our neural network at a time, until the entire training set is passed through. 32 is the default\n","batch_size = 32\n","\n","# Here we set the size of all the images to be 200x200\n","img_height = 200\n","img_width = 200\n","\n","train = tf.keras.utils.image_dataset_from_directory(\n","  data_dir,\n","  validation_split = 0.2,\n","  subset = \"training\",\n","  seed = 1234,\n","  image_size = (img_width, img_height),\n","  batch_size = batch_size,\n","  labels = \"inferred\",\n","  label_mode = \"categorical\",\n","  color_mode = \"rgb\"\n",")\n","\n","valid = tf.keras.utils.image_dataset_from_directory(\n","  data_dir,\n","  validation_split = 0.2,\n","  subset = \"validation\",\n","  seed = 1234,\n","  image_size = (img_height, img_width),\n","  batch_size = batch_size,\n","  labels = \"inferred\",\n","  label_mode = \"categorical\",\n","  color_mode = \"rgb\"\n",")\n","\n","class_names = train.class_names\n","print(class_names)\n","num_classes = len(class_names)\n","print(num_classes)\n","\n","first_batch = valid.take(1)\n","\n","for img, lbl in first_batch:\n","    for i in np.arange(1):\n","        shape = img[i].shape\n","\n","print(shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Building and Training CNN"]},{"cell_type":"markdown","metadata":{},"source":["#### Simple CNN Model 1 w/ Adam Optimizer 50 Epochs"]},{"cell_type":"code","execution_count":null,"id":"9e07bf64-3192-431b-aceb-4e38196eb3ad","metadata":{},"outputs":[],"source":["# Let's build a CNN using Keras' Sequential capabilities\n","\n","model_1 = Sequential()\n","\n","\n","## 5x5 convolution with 2x2 stride and 32 filters\n","model_1.add(Conv2D(32, (5, 5), strides = (2,2), padding='same',\n","                 input_shape=shape))\n","model_1.add(Activation('relu'))\n","\n","## Another 5x5 convolution with 2x2 stride and 32 filters\n","model_1.add(Conv2D(32, (5, 5), strides = (2,2)))\n","model_1.add(Activation('relu'))\n","\n","## 2x2 max pooling reduces to 3 x 3 x 32\n","model_1.add(MaxPooling2D(pool_size=(2, 2)))\n","model_1.add(Dropout(0.25))\n","\n","## Flatten turns 3x3x32 into 288x1\n","model_1.add(Flatten())\n","model_1.add(Dense(512))\n","model_1.add(Activation('relu'))\n","model_1.add(Dropout(0.5))\n","model_1.add(Dense(num_classes))\n","model_1.add(Activation('softmax'))\n","\n","model_1.summary()\n","model_1.compile(loss = tf.keras.losses.CategoricalCrossentropy(), optimizer = 'adam', metrics = ['accuracy'])\n","model_1.fit(train, validation_data = valid, epochs = 50, batch_size = batch_size)"]},{"cell_type":"markdown","id":"6f7440af-98ed-4c33-8e1d-053e61aede29","metadata":{},"source":["We still have 181K parameters, even though this is a \"small\" model.\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Simple CNN Model 2 w/ RMSprop Optimizer 100 Epochs"]},{"cell_type":"code","execution_count":null,"id":"7e06badf-922e-41d3-ae63-4ef4b8f939ad","metadata":{},"outputs":[],"source":["# Let's build a CNN using Keras' Sequential capabilities\n","\n","model_2 = Sequential()\n","\n","\n","## 5x5 convolution with 2x2 stride and 32 filters\n","model_2.add(Conv2D(32, (5, 5), strides = (2,2), padding='same',\n","                 input_shape=shape))\n","model_2.add(Activation('relu'))\n","\n","## Another 5x5 convolution with 2x2 stride and 32 filters\n","model_2.add(Conv2D(32, (5, 5), strides = (2,2)))\n","model_2.add(Activation('relu'))\n","\n","## 2x2 max pooling reduces to 3 x 3 x 32\n","model_2.add(MaxPooling2D(pool_size=(2, 2)))\n","model_2.add(Dropout(0.25))\n","\n","## Flatten turns 3x3x32 into 288x1\n","model_2.add(Flatten())\n","model_2.add(Dense(512))\n","model_2.add(Activation('relu'))\n","model_2.add(Dropout(0.5))\n","model_2.add(Dense(num_classes))\n","model_2.add(Activation('softmax'))\n","\n","model_2.summary()\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model_2.compile(loss = 'categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","model_2.fit(train, validation_data = valid, epochs = 100, batch_size = batch_size)"]},{"cell_type":"markdown","metadata":{},"source":["#### Advanced CNN Model"]},{"cell_type":"code","execution_count":null,"id":"78ce2d42-e633-4e1c-9f29-7714d1308df4","metadata":{},"outputs":[],"source":["# Let's build a CNN using Keras' Sequential capabilities\n","\n","model_3 = Sequential()\n","\n","model_3.add(Conv2D(32, (3, 3), padding='same',\n","                 input_shape=shape))\n","model_3.add(Activation('relu'))\n","model_3.add(Conv2D(32, (3, 3)))\n","model_3.add(Activation('relu'))\n","model_3.add(MaxPooling2D(pool_size=(2, 2)))\n","model_3.add(Dropout(0.25))\n","\n","model_3.add(Conv2D(64, (3, 3), padding='same'))\n","model_3.add(Activation('relu'))\n","model_3.add(Conv2D(64, (3, 3)))\n","model_3.add(Activation('relu'))\n","model_3.add(MaxPooling2D(pool_size=(2, 2)))\n","model_3.add(Dropout(0.25))\n","\n","model_3.add(Flatten())\n","model_3.add(Dense(512))\n","model_3.add(Activation('relu'))\n","model_3.add(Dropout(0.5))\n","model_3.add(Dense(num_classes))\n","model_3.add(Activation('softmax'))\n","\n","model_3.summary()\n","\n","# initiate RMSprop optimizer\n","opt_2 = keras.optimizers.RMSprop(lr=0.0005)\n","\n","# Let's train the model using RMSprop\n","model_3.compile(loss='categorical_crossentropy', optimizer=opt_2, metrics=['accuracy'])\n","model_3.fit(train, validation_data = valid, epochs = 150, batch_size = batch_size)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
